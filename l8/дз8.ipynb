{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "a401320c358d1cf36ae00081533963c9.ipynb (1).txt",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCdGFvtyItIK"
      },
      "source": [
        "# GAN overriding `Model.train_step`\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2019/04/29<br>\n",
        "**Last modified:** 2020/04/29<br>\n",
        "**Description:** A simple DCGAN trained using `fit()` by overriding `train_step`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZn7_uCAItIM"
      },
      "source": [
        "## Загрузка модулей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPbdfnz4ItIO"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eL0jLVLItIV"
      },
      "source": [
        "## строим MNIST data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_5cxeh_ItIW"
      },
      "source": [
        "# MNIST \n",
        "batch_size = 64\n",
        "(x_train, _), (x_test, _) = keras.datasets.fashion_mnist.load_data()\n",
        "all_digits = np.concatenate([x_train, x_test])\n",
        "all_digits = all_digits.astype(\"float32\") / 255\n",
        "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(32)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9GINZz5ItIb"
      },
      "source": [
        "## Строим discriminator\n",
        "\n",
        "размер карты 28x28 и бинарная классификация (настоящее изображение или генерировано)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K99eZdkgItIc",
        "outputId": "abc58be1-bd32-41c3-b203-61dadbf7ee30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, (3, 3), strides=(1, 1), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "\n",
        "discriminator.summary()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_39 (Conv2D)           (None, 14, 14, 64)        640       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_64 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_65 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_66 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d_12 (Glo (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 222,209\n",
            "Trainable params: 222,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eodbCQ1WItIh"
      },
      "source": [
        "## Строим generator\n",
        "\n",
        "обратное по отношению к дискриминатору преобразование, меняем `Conv2D` на `Conv2DTranspose` ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY5lCMxqItIi",
        "outputId": "29e23e75-4793-421c-c0ba-2f48a2ead98e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        # строим размер входного вектора 7x7x128 map\n",
        "        layers.Dense(7 * 7 * 128),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Reshape((7, 7, 128)),\n",
        "        layers.Conv2DTranspose(128, (3, 3), strides=(1, 1), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "\n",
        "generator.summary()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_25 (Dense)             (None, 6272)              809088    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_71 (LeakyReLU)   (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "reshape_12 (Reshape)         (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_28 (Conv2DT (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_72 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_29 (Conv2DT (None, 14, 14, 128)       147584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_73 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_30 (Conv2DT (None, 28, 28, 128)       147584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_74 (LeakyReLU)   (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_43 (Conv2D)           (None, 28, 28, 1)         6273      \n",
            "=================================================================\n",
            "Total params: 1,258,113\n",
            "Trainable params: 1,258,113\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpX7dJvAItIn"
      },
      "source": [
        "## Класс со своим этапом обучения `train_step`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHP6aHUfItIo"
      },
      "source": [
        "\n",
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        if isinstance(real_images, tuple):\n",
        "            real_images = real_images[0]\n",
        "        # берем случайный пример из скрытого пространства\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Строим по нему фейковое изображение\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # собрали с реальным в текзор\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # задаем метки 1 и 0 соответственно\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Добавляем шум !!!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # учим discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        #Выбрали случайный пример в скрытом пространстве\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # собрали метки реальных изображений\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Учим generator !\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4rYDb3qItIs"
      },
      "source": [
        "## Callback для сохранения изображений по ходу обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoLCvAe7ItIt"
      },
      "source": [
        "\n",
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        for i in range(self.num_img):\n",
        "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiMWOk1_ItIz"
      },
      "source": [
        "## Учим end-to-end модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZSj0hUHItI0",
        "outputId": "7f17c444-028c-41e7-a306-ff9b8cad3760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        ")\n",
        "\n",
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=3, latent_dim=latent_dim)]\n",
        ")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1094/1094 [==============================] - 69s 63ms/step - d_loss: 0.3861 - g_loss: 5.4727\n",
            "Epoch 2/10\n",
            "1094/1094 [==============================] - 69s 63ms/step - d_loss: 0.1491 - g_loss: 4.8776\n",
            "Epoch 3/10\n",
            "1094/1094 [==============================] - 69s 63ms/step - d_loss: 0.6171 - g_loss: 1.2167\n",
            "Epoch 4/10\n",
            "1094/1094 [==============================] - 69s 63ms/step - d_loss: 0.5606 - g_loss: 1.3425\n",
            "Epoch 5/10\n",
            "1094/1094 [==============================] - 69s 63ms/step - d_loss: 0.5799 - g_loss: 1.2336\n",
            "Epoch 6/10\n",
            "1094/1094 [==============================] - 69s 63ms/step - d_loss: 0.5584 - g_loss: 1.2293\n",
            "Epoch 7/10\n",
            "1094/1094 [==============================] - 69s 63ms/step - d_loss: 0.5504 - g_loss: 1.2803\n",
            "Epoch 8/10\n",
            "1094/1094 [==============================] - 69s 63ms/step - d_loss: 0.5678 - g_loss: 1.2507\n",
            "Epoch 9/10\n",
            "1094/1094 [==============================] - 69s 63ms/step - d_loss: 0.5486 - g_loss: 1.3208\n",
            "Epoch 10/10\n",
            "1094/1094 [==============================] - 69s 63ms/step - d_loss: 0.5237 - g_loss: 1.3637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f29fcb74128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg-EpNB85MAH",
        "outputId": "82cbb0a0-37e6-408d-e773-87d8f4b35d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=3, latent_dim=latent_dim)]\n",
        ")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1094/1094 [==============================] - 67s 61ms/step - d_loss: 0.6190 - g_loss: 0.9921\n",
            "Epoch 2/3\n",
            "1094/1094 [==============================] - 67s 61ms/step - d_loss: 0.5998 - g_loss: 1.0484\n",
            "Epoch 3/3\n",
            "1094/1094 [==============================] - 67s 61ms/step - d_loss: 0.5682 - g_loss: 1.0866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2a02af1c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKqdR6bOItI4"
      },
      "source": [
        "Display the last generated images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZa20XZYItI5",
        "outputId": "80da988a-5085-4099-94dd-387b21c83da1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(\"generated_img_0_2.png\"))\n",
        "display(Image(\"generated_img_1_2.png\"))\n",
        "display(Image(\"generated_img_2_2.png\"))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACj0lEQVR4nE3SSUwTYRQH8DfTmW601HbSJQEbgbJpCRwUg2twiaJcNHrQePIi3vRmTLzgdiFRDiq4kCARDcULIQSNICFEIGlqlGKNBVqbspTFzrR2meX7Pg9C6bu+/PLPWygaAwAAY9TbOAtPXGgsSnQ4gQEAKAAAAI324XyttSYkHFZHOhvrvT3vAAAoigCA2n69RrdYRcdWThv5SVe1FusUAIYhAECRs8FLPjuutNvsdMEelwZUGAAUiiKgVT9ljZF1c6Hqq61ZnZ6mD5VRLAIAhgCIXO8JudYQPc8cHSonqONOgZRFAAAMAOh1VUNO2Y9KuKnwEbUv2XPSrYGtZgbJu4ubTk3Vc26Pk3X0XuFokmuS2KSDT443TJv6HPMwZvpQfGxzQgAArbv5me/LXT4evSVm/7hSacznJBB1NIzmUmNkMPRZ3Sl/3FtUCLkNsaaS0pvwvdGQenFbl64NMgB0TmK9qJkI1AW03VxI02bxOy2QV7S22uPtSyXWnygoswtjjLYzgYjxUafdjyaWNlQeq2Ck6XxKGToC44uxuXYJZawIE5wnAah+7/FV0u+I0V1FYauBypfsjtGVWVFYeoRQVi/JOF+ypG4+Y/qVeimuqjq4gQp3nrToi8wBYSUu/LwvIdG4lN2W9L6IPVrVUnHZ7GsvTSqvKgerD+bcwOPOcp3hQWhaSYSvISRb48mcZF/vnAkr+vcj9atr91RBpt/ZdWD/Via1vDzcYLKMZCKSEGlRkFw2y29KGizP26I/QP/23O9vwaY4nx65MDyTizRnpNREqyepCFiW+jBGF/m1rUz6TEyxxGw24a/Cin5jBoSrfhO3eQ/yaSM+98bbuhBfEBeZgYQg3+jpTv//oX8GyTSYGm/lFQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACyklEQVR4nD3QW0iTYRzH8f/z7N3pnTaXmjrnNpeleChb2oVRWHaUpFQQBKOC0BIvpC66kqibQILArrpKsIOhqdiFEKWlTgdaoeXmXOrUpTvou03ntnfvYV3s1esvH37P/0EyGmKAMHGAVCXL3dGynb4QYFmEBwBABIdiItDhF4uGLP3ayhXkuXS2zem6BgBAACAFUZBaBSccmuUFe7lUWmdQHgLMAyCEYzJpQeS2rW0FpagjK4UA04UyAAwAgMRYcWe3ep3weQ2J5KCyDUc6TY9KhIhBm3FdpEgsjczUAf+qFcDYrZXFI4HQ9tVN6Yhx+3dIo+obvYusCc3tRgAAAIJHGm8ZStJW1EyWSB46E+DkQu9xECIWRY/NLUlCG/2V8oPt0qDIcfrpjeZ4RBhXXda7vreko94mJtwwBHyiU5wgbGI5Ra6NLf5ItDJTwXf/7HhQ9awxBwlS/PiIzmwur8QTZeBqHBNzSR4SAwIAIAhtQOnsF3tGhytmyY5Mt3y48FNxHhJelGFesz451Tnv/LBLOWpZjlVyHM/Hm0hrZ7xffA55OHnBP8Q4iVH1okYeh5hdzTNk6jmT/mJq8uEaaVp6tSNbun9nnoWihnluIFbuD/WCC/fqzNlqYVOUvxz8c0FV+sb+NeixNLEcTbLs3ibWLHtddoaYC+jn3d3+dfGA3qpXCBJr7YGpM6q0ng2r2/PrHs3RSSE2FotLosL6l7Jx5FtTJWV7neqPDeaO5OsEKT63Ff5ZnKB+bpmgffP1HMck0fT+nYUuj2WNISd9qujqy/CC5FvBqFElSGRY8nTkp6S+37DuUNZ6hovm7DB7Eum6ZrK35BLTyM0NvjXXpZit/XheI/wCLlpeHW+ov2Xx2dgg9YDjuGo6sielR2c2s9RM0SYdIMMjmds42rKulAiQGbfbFj+7u6gtP97JmQ4FA/e7egIsAAD8B1/sP4X7+aa3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAADH0lEQVR4nDXPS0wTeRzA8d8M/07fD4fS0hfPQi2otAh1xV2DIqIbdjXGmBgTE403NdGTN2O8bjRGvWxWDyYmGg24GrMRsMZHEVBEHotooUCxLW3HPseZaWc6nT3s7vf6OX2xY7FwXJTpebGk0VnKcjkzWoT/Q52F0AivODKpSnSp965pnPL+oISX/0Oj2Q6Rjd4fAz7jNvAw1djJy7hWoAAAANcaaqxdtHvFGyXnlPEyofZYd9zqJQEAAPu9Ix8NOqqVfrDXZSSdV0//0b1VM+UrAwBOx4px2WRpXp+O86w0hjLFiYTAjZcBALCzzmKaW1O4o9M2X+kztlubC3jbOoEsAADCzTrqm0UvNm+le4L1fjdreepqIGIYAAAq8EBlAq3WRf8vfn7IMGCU6iJ0F05wAIAEfaP3izPVVzy0dHDJNtCkI6g2Azn/72eUSUTWX7Et+fmGSGpGHa4qa9dLVo8pBwBoMdiukdx00mtjjVhbQdFIMQaLjNMAACBBHgvpBpt0r8d/XRFyOvM3vkzltMr+jwCABMLTRbFzrir3am2qKVlhyUZMSlidAwBAeDXFckMpa2jR/j63nKfSEeZdJarTYhIAYid+lifbQ/FeN9VIGSdsta5ASyOZdSMBABE/Rb5W3jO6RmZ3y7IfCO7jJ1iNkq2dMgEAlZL9rflzgQ7b/rVdiZbRHxTb/+xFYtylYfEyQuoFXHk3b819sYdSfi6ETSfGqpia51tGeQx9D+wzpI4MRzs2ZU2kasph3/SwxQRrjrpogx8pamflG66SlsdLB6pXHtgTiaeaF5Zg/ft59RIG2hOzUjqwf+Svl9eY5benM5nYBSq2+GjlwtHHzQjCg/ni5fizdFLRvDCceSYL04O1KscdvPIdh9jgYc/Cld8aNjuXfY6dt3cppesbHTTIzUOnnmAVsu3de877qPbB40TNJWfP5jGY6TUYbtBN5E0MU5y5iBdmtohErkZk3/RV8MP1REp1388euoswbHzaxqgYkeO5YkqTk7HkZ7tCDMoTQywCXhjomRIN5nVbVpr6O28KP0o7Zn3jIlel/gcntWpr1/0vuQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WruOaC0ahMZr",
        "outputId": "7ecdfbf3-40c2-4eed-ddaa-8b44e00c43e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "display(Image(\"generated_img_0_9.png\"))\n",
        "display(Image(\"generated_img_1_9.png\"))\n",
        "display(Image(\"generated_img_2_9.png\"))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABm0lEQVR4nGNgGPTAhhuXDHOd/CtNUaanrAwM6t0MLKiSfzfLXvqgGdQpIqrBepSBCU2rudLFe/vt7yev0ZDIYGBElWOSFVqr8GeyjfgDFo6z6Wg62f++m/DqpcnrvceleX+g65RXv8rCfpdBPtbk0rvt19B0FsjxPrn19++fMO7t59ahubbY24jn/UsGRvb1/zvfoPiXjSfozeuHtx2YGBgdl9758OoPTILj0R92p79/Xt199/+lBhPf5S83+5RmQh3Erv/jJtOxf7f+yfKq/PE8w+pvw7RGaL4QCwMDAwPjktOMz993+Nw3/3TxsaDXa0nrnzZyR+sZWViYf3HFW2n8FLuef8jmmZoI0wvP9+Z8v9/euSgoyqgwyUjgjgiLwNn7yl++yvB93XXGU8ia5RXPqaM/GVmEjV7cYn7FxvKC6SGvwIfzbc//nrVi4/vP8OWf5EuWC8Gyj2V0TXTYmP/Indw/+xsDwyPRe5aiDNrv3n5k+Xvy7q8XTzg5/97mkL0/9xsDA8OfC8oSd39/+/poA/Y4Z8QuTA8AAIeHoD6FxSPnAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACT0lEQVR4nG2SS2sTURSAz517b+7MZJJJWk0aCpZiUWsXFW1dFKsVsStd6NJ/YVf+AQXXdiEoRVcu3BRcuOimgqAgQimCotRHW/uwSUxmJjNzb+7DhZMI6tl+5zvf5gD8GTYvjHiC4D9z94eKQi2ip7P/siG5e394Pbw5e/F29W/5dfJobGpb6ccTi+/m7i0zAADIluZW1Pv6sdKExOeOjF4eKD67DgBgAQCAtfjmwY7Hr6wuedcmzdLb5AQAABAAALjgk9MDTfdOu/Gw+PPD8WFNkembC6Kbqs63xu7MaBpVKk7XH+yfLbGwY8NImnxd29hsTqck+H6mD6epXRupfHLtWpXoL68It9xL/ebVQ8Spe4ZQTELujzOkDs73ID2Zhh6yEr1HNSokcd5RPu6dZTIdJIVWtRmHMkrsmraituyZzC9iN/Z2DsucIIaKliXZUM+MPgvZ3WrRHaEgp5G9bZAwVmaaoz5WbYYodZAxaWBYTuRY8tu0SRrXC6k2SZBwiw6WAEd8MjNRoYyiaiC49DEnxdiVuJpnWdPaa4abPCG4aBFsKUU4DtV8BiuqYWSqZMF1sYctUZLAKM7gKB2iayaKA+homRM2IjnGy1mzVSrimRrmHVZAUpaFY5BrbWVm8wUkG92O41MAYgEiqeKa95pTNmWsBK5yWQEs5HRDal5m8ChH9edG5ZSjsWDMF6zLuu2s6YzrW7GmgUzKgZ+wqHiwkT+7mcEbXrJa3vcGpCR5J03zUbz+Me5krzm2vLBin8I2bniS7Td5rKWxYwD4BWoLC2WSUaCPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACA0lEQVR4nM3STU8TURQG4PfeuTN3OjOlLdOmNCYl1QgIQVINkYUfRMUEE6MucEGI0cS1O1my0bj1R6gxLkjYuZDEjU1TcQGkhEgToklpDbVAy0yZTu90XMiHRH6A7+a8Oc/u5AD/fSbYYaWHTdofW+IEPAcA4B1Hm78wDwCkh4Yy6j9IANBHzoXBgc0r4ZnnkkLpAcocBPGJqVKZG6o5eiM3eq/0DgQAEB2YXq1svK7OVGJP7KXZrquNsVL5ZeEPJqcuJXq9hafWs82RcovspXlt7uzgOAPxYT683d5J3X3QM5IaMmSlwn750UnLIAR3PsRvTnNztvG4ku123YYXq4TbIeFGEn1UfnP9WsfaNkln17ThMAKdcl0QUwvtfV3uZWNM42fqysdMPLG1W6VUVTSPO55bpMk+aomknbdWokYpk/8msaZl6LpE2uH+yBePrb+Y1yfDQWk40FUwmUp8VxPcl4SlqEN049UPr+AZyq780/e4HCG+IIQTvbTdCDJfiGWFLnQ7/jrRyjq3G7Tp14WfKmqfGQCxqJ42AjEiqU34Lcl2i8Fqa+nTYpkBgDh1sd+qC5XRwK7u6Ihyw12dd8AAoB2PBbM2sRNxSd3pdETufD733sP+4bl5n8ZqK5eLtfAt1WPf57KWhwME1BYgtQXR0rIx/jZ39CrHQohk0pPpeH4DnC6/R61OdVMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-o1cIfLhXhq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH5oTIilFQ1t"
      },
      "source": [
        "Изменение сложности сверточных слоев позволило немного улучшить качество создаваемого изображения.  Лучшее изображение выглядело следующим образом\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpTh8oJpF2PN"
      },
      "source": [
        "Возникли трудности с усложнением модели. Я старался добавить по 1 скрытому слою в дискриминатор и генератор. Это оченьб сильно усложняло модель и очень плохо сказывалось на качестве, иногда сети просто не обучались.  Проблема была в том, что невозможно было уменьшать размер изображения, более 2х раз. На первой итерации размер был 14*14, потом 7*7,а вот последующее уменьшение приводило к потере данных. Я пробовал использовать по 1 дополнительному слою, но без уменьшения размерности на 1 из слоев, но это давало ужасные результаты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I7HPyYQIJpX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_owlC4_GELl"
      },
      "source": [
        "Увеличение количества эпох очень хорошо улучшало качество рабочей сети и выдаваемое изображение. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymMFFfzhVxQd"
      },
      "source": [
        "Затем я перешел на fashion_mnist. Сеть вообще вела себя странно, за 3 эпохи она могла выдать очень красивое платье, а если переучить ее снова, то за 5 вообще что попало выдавало. Не совсем поянл, как это устроено. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLcBoQA0V_8D"
      },
      "source": [
        "Начал эксперименты с добавлением дополнительного слоя в генератор. Добавил 1 слой в генератор, результат плохой. Добавил еще слой в дискриминатор, результат тоже плохой оказался. Код оставил последней попытки, может будут какие-то советы, что можно попытаться сделать, чтобы улучшить сеть? Оставил в ноутбуке самую удачную картинку из fashion mnist. В какой-то момент она была похожа на рубашку, но потом что-то переобучилось? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlFnDOzroUb6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj_sOg2eaDLw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfXoFN6dV_IU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkQ6Wrg8GP8K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44zM0JwNF0kc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5XKqGiiFu2f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}